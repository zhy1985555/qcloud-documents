>? TDSQL-C Mysql 数据源、PostgreSQL 数据源、SQL Server 数据源、Oracle 数据源、DB2数据源、DM 数据源、SAP HANA 数据源、Clickhouse 数据源、Greenplum 数据源、GaussDB 数据源、Tbase 数据源的参数说明请查看**通用数据源**。

<dx-accordion>
::: 通用数据源
#### 读取

| 参数 | 说明 | 
|---------|---------|
| <nobr>数据源	| 选择已经配置好的目标数据源（如果未配置数据源，请单击右侧的**新建数据源**，进入**项目管理 > 数据源管理**页面进行新建）<br> ![](https://qcloudimg.tencent-cloud.cn/raw/f2d87334c968384e7c45bca7cfc1d830.png)| 
| 库	| 选取需要同步的目标数据库| 
| 表	| 选取需要同步的目标表名称| 
| 切割键	| 选填：执行分库分表的切割键, 指定用于数据分片的字段，指定后将启动并发任务进行数据同步| 
| <nobr>筛选条件	| 选填：使用 where 条件进行数据过滤，请根据 SQL 语法填写对应语句。在实际业务场景中，可选择当天的数据进行同步，将 where 条件指定为 gmt_create>${yyyymmdd}（未配置过滤条件的情况下，数据同步均视作同步全量数据）| 

#### 写入

| 参数 | 说明 | 
|---------|---------|
| 数据源	| 选择已经配置好的目标数据源（如果未配置数据源，请单击右侧的**新建数据源**，进入**项目管理 > 数据源管理**页面进行新建）| 
| 库	| 选取需要同步的目标数据库| 
| 表	| 选取需要同步的目标表名称| 
| 是否清空表	| 用户可以选择是否清空当前写入表，若选择是，则当前表中全部数据会被清空| 
| 写入模式	| 数据写入方式：<li>Append 追加数据：主键/唯一性索引冲突时,会先删除原有行，再插入新行<li>Overwrite 覆盖数据：replace into 写入；主键/唯一性索引冲突时,会先删除原有行再插入新行, 相当于更新原行所有字段<li>On duplicate key：主键/唯一性索引冲突时,新行会替换已指定的字段的语句| 
|  <nobr>批量提交大小	| 一次性批量提交的记录数大小，默认为1024条| 
| 前置 SQL	| 执行数据同步任务之前率先执行的 SQL 语句，语句需要遵循 SQL 语法| 
| 后置 SQL	| 执行同步任务之后执行的 SQL 语句, 语句需要遵循 SQL 语法| 
:::
::: Mysql 数据源
#### 读取

| 参数 | 说明| 
|---------|---------|
| 数据源 | 选择已经配置好的 MySQL 数据源 （如果未配置数据源，请单击右侧的**新建数据源**，进入**项目管理 > 数据源管理**页面进行新建）<br>![](https://qcloudimg.tencent-cloud.cn/raw/f2d87334c968384e7c45bca7cfc1d830.png)| 
| 库	| 选取需要同步的目标数据库| 
| 表	| 选取需要同步的目标表名称| 
| 切割键	| 选填：执行分库分表的切割键, 指定用于数据分片的字段，指定后将启动并发任务进行数据同步| 
| <nobr>筛选条件	| 选填：使用 where 条件进行数据过滤，请根据 SQL 语法填写对应语句。在实际业务场景中，可选择当天的数据进行同步，将 where 条件指定为 gmt_create>${yyyymmdd}（未配置过滤条件的情况下，数据同步均视作同步全量数据）| 


#### 分库分表
通过单击**分库分表**，可以选择多个不同数据库下需要执行数据读取任务的库表，每次点击都会生成新的库表选择框，但分库分表需要保证所有表 schema 一致。

#### 写入

| 参数 | 说明| 
|---------|---------|
| 数据源	| 选择已经配置好的 MySQL 数据源（如果未配置数据源，请单击右侧的**新建数据源**，进入**项目管理 > 数据源管理**页面进行新建）| 
| 库	| 选取需要同步的目标数据库| 
| 表	| 选取需要同步的目标表名称| 
| 是否清空表	| 用户可以选择是否清空当前写入表，若选择是，则当前表中全部数据会被清空| 
| 写入模式	| 数据写入方式：<li>Append 追加数据：主键/唯一性索引冲突时,会先删除原有行，再插入新行<li>Overwrite 覆盖数据：replace into 写入；主键/唯一性索引冲突时,会先删除原有行再插入新行, 相当于更新原行所有字段<li>On duplicate key：主键/唯一性索引冲突时,新行会替换已指定的字段的语句| 
|  <nobr>批量提交大小	| 一次性批量提交的记录数大小，默认为1024条| 
| 前置 SQL	| 执行数据同步任务之前率先执行的 SQL 语句，语句需要遵循 SQL 语法| 
| 后置 SQL	| 执行同步任务之后执行的 SQL 语句, 语句需要遵循 SQL 语法| 



:::
::: Hive 数据源
#### 读取

| 参数 | 说明 | 
|---------|---------|
| 数据源	| 选择已经配置好的 Hive 数据源（如果未配置数据源，请单击右侧的**新建数据源**，进入**项目管理 > 数据源管理**页面进行新建）<br>![](https://qcloudimg.tencent-cloud.cn/raw/f2d87334c968384e7c45bca7cfc1d830.png)| 
| 库	| 选择当前项目下已经配置 Hive 数据源下的需要同步的单个数据库| 
| 表	| 选取需要同步的表名称，一个数据同步任务只能同步一张表| 
| 读取方式	| 数据来源的读取方式，目前仅支持 JDBC 连接串| 
| <nobr>筛选条件	| 选填：使用 where 条件进行数据过滤，请根据 SQL 语法填写对应语句。在实际业务场景中，可选择当天的数据进行同步，将 where 条件指定为 gmt_create>${yyyymmdd}（未配置过滤条件的情况下，数据同步均视作同步全量数据）| 

####  写入

| 参数 | 说明 | 
|---------|---------|
| 数据源	| 选择已经配置好的 Hive 数据源（如果未配置数据源，请单击右侧的**新建数据源**，进入**项目管理 > 数据源管理**页面进行新建）| 
| 库	| 选取需要同步的目标数据库| 
| 表	| 选取需要同步的目标表名称| 
| 写入模式	| 数据写入方式：<li>Append 追加数据：主键/唯一性索引冲突时,会先删除原有行，再插入新行<li>Overwrite 覆盖数据：replace into 写入；主键/唯一性索引冲突时,会先删除原有行再插入新行, 相当于更新原行所有字段<li>On duplicate key：主键/唯一性索引冲突时,新行会替换已指定的字段的语句| 
|  <nobr>批量提交大小	| 一次性批量提交的记录数大小，默认为1024条| 
| 空字符串 | 数据同步过程中对空字符串的处理方式：<li>不做处理：当源表字段为空时不做处理，目标字段内容将为空<li>处理为 null：当源表字段为空时默认处理为 null 写入目标字段| 

:::
::: HBase 数据源
#### 读取

| 参数 | 说明 | 
|---------|---------|
| 数据源	| 选择已经配置好的 HBase 数据源（如果未配置数据源，请单击右侧的**新建数据源**，进入**项目管理 > 数据源管理**页面进行新建）<br>![](https://qcloudimg.tencent-cloud.cn/raw/f2d87334c968384e7c45bca7cfc1d830.png)| 
| 库	| 选择当前项目下已经配置 HBase 数据源下的需要同步的单个数据库| 
| 表	| 选取需要同步的表名称，一个数据同步任务只能同步一张表| 
| <nobr>读取模式	| 读取数据的模式，横标或竖表。<li>横表：将 Hbase 表当成普通二维表（横表）进行读取,读取最新版本数据 <li>竖表：将 HBase 中的表当成竖表进行读取，读出的每条记录为四列形式（rowKey，family:qualifier，timestamp，value），读取最新版本数据| 

####  写入

| 参数 | 说明 | 
|---------|---------|
| 数据源	| 选择已经配置好的 HBase 数据源（如果未配置数据源，请单击右侧的**新建数据源**，进入**项目管理 > 数据源管理**页面进行新建）| 
| 命名空间	| HBase的命名空间| 
| 表	| 选取需要同步的目标表名称| 
| ro <nobr>wkey 规则	| 在**点击配置**页面，单击**添加一行**<br>![](https://qcloudimg.tencent-cloud.cn/raw/0262376fb855705844f7a4fcd003374a.png) <br>支持配置 random(n)值、Ipad(expe、len、pad)值、reverse(expr)值、MD5(expr)值、常量值、源表列值| 

:::
::: DLC 数据源
#### 读取

| 参数 | 说明 | 
|---------|---------|
| <nobr>数据源	| 选择已经配置好的 DLC 数据源（如果未配置数据源，请单击右侧的新建数据源，进入**项目管理 > 数据源管理**页面进行新建）<br>![](https://qcloudimg.tencent-cloud.cn/raw/f2d87334c968384e7c45bca7cfc1d830.png)| 
| 库	| 选取需要同步的目标数据库| 
| 表	| 选取需要同步的目标表名称| 
| 切割键	| 选填：执行分库分表的切割键, 指定用于数据分片的字段，指定后将启动并发任务进行数据同步| 
| <nobr>筛选条件	| 选填：使用 where 条件进行数据过滤，请根据 SQL 语法填写对应语句。在实际业务场景中，可选择当天的数据进行同步，将 where 条件指定为 gmt_create>${yyyymmdd}（未配置过滤条件的情况下，数据同步均视作同步全量数据）| 

#### 写入

| 参数 | 说明 | 
|---------|---------|
|  <nobr>数据源	| 选择已经配置好的 DLC 数据源（如果未配置数据源，请单击右侧的**新建数据源**，进入**项目管理 > 数据源管理**页面进行新建）| 
| 库	| 选取需要同步的目标数据库| 
| 表	| 选取需要同步的目标表名称| 

:::
::: Kudu 数据源
#### 读取

| 参数 | 说明 | 
|---------|---------|
| 数据源	| 选择已经配置好的 Kudu 数据源（如果未配置数据源，请单击右侧的新建数据源，进入**项目管理 > 数据源管理**页面进行新建）<br>![](https://qcloudimg.tencent-cloud.cn/raw/f2d87334c968384e7c45bca7cfc1d830.png)| 
| 库	| 选择当前项目下已经配置 Hive 数据源下的需要同步的单个数据库| 
| 表	| 选取需要同步的表名称，一个数据同步任务只能同步一张表| 
| 读取方式	| 数据来源的读取方式，目前仅支持 JDBC 连接串| 
| <nobr>筛选条件	| 选填：使用 where 条件进行数据过滤，请根据 SQL 语法填写对应语句。在实际业务场景中，可选择当天的数据进行同步，将 where 条件指定为 gmt_create>${yyyymmdd}（未配置过滤条件的情况下，数据同步均视作同步全量数据）| 

####  写入

| 参数 | 说明 | 
|---------|---------|
| 数据源	| 选择已经配置好的 Kudu 数据源（如果未配置数据源，请单击右侧的**新建数据源**，进入**项目管理 > 数据源管理**页面进行新建）| 
| 库	| 选取需要同步的目标数据库| 
| 表	| 选取需要同步的目标表名称| 
| 写入模式	| 数据写入方式：<li>Append 追加数据：主键/唯一性索引冲突时,会先删除原有行，再插入新行<li>Overwrite 覆盖数据：replace into 写入；主键/唯一性索引冲突时,会先删除原有行再插入新行, 相当于更新原行所有字段<li>On duplicate key：主键/唯一性索引冲突时,新行会替换已指定的字段的语句| 
|  <nobr>批量提交大小	| 一次性批量提交的记录数大小，默认为1024条| 

:::
::: HDFS 数据源
#### 读取

| 参数 | 说明 | 
|---------|---------|
| 数据源	| 选择已经配置好的 HDFS 数据源（如果未配置数据源，请单击右侧的**新建数据源**，进入**项目管理 > 数据源管理**页面进行新建）<br>![](https://qcloudimg.tencent-cloud.cn/raw/f2d87334c968384e7c45bca7cfc1d830.png)| 
| 文件路径	| 要读取的文件路径。如要读取多个文件，可用逗号（,）分割，或用正则表达式通配，例如/hadoop/data_20210201/*。支持${yyyymmdd}时间变量参数| 
| 源文件格式	| 支持 ORC、TEXT、PARQUET、CSV| 
| 压缩格式	|支持压缩格式 deflate、gzip、bzip2、lz4、snappy| 
| <nobr>字段分隔符	| 读取的字段分隔符。支持\t、\u0001（Hive 默认）、竖线、空格、;（分号）、,（逗号）| 
| 编码	| 读取文件的编码配置，支持 utf-8、gbk| 

####  写入

| 参数 | 说明 | 
|---------|---------|
| 数据源	| 选择已经配置好的 HDFS 数据源（如果未配置数据源，请单击右侧的**新建数据源**，进入**项目管理 > 数据源管理**页面进行新建）| 
| 目标文件	| 要写入的文件路径。如要读取多个文件，可用逗号（,）分割，或用正则表达式通配，例如/hadoop/data_20210201/*。支持${yyyymmdd}时间变量参数| 
| 写入模式	| 数据写入方式：<li>Append 追加数据：主键/唯一性索引冲突时,会先删除原有行，再插入新行<li>Overwrite 覆盖数据：replace into 写入；主键/唯一性索引冲突时,会先删除原有行再插入新行, 相当于更新原行所有字段<li>On duplicate key：主键/唯一性索引冲突时,新行会替换已指定的字段的语句| 
| 文件类型	| 支持 ORC、TEXT、PARQUET、CSV| 
| 压缩格式	|支持压缩格式 deflate、gzip、bzip2、lz4、snappy| 
| <nobr>字段分隔符	| 读取的字段分隔符。支持\t、\u0001（Hive 默认）、竖线、空格、;（分号）、,（逗号）| 

:::
::: COS 数据源
#### 读取

| 参数 | 说明 | 
|---------|---------|
| 数据源	| 选择已经配置好的 COS 数据源（如果未配置数据源，请单击右侧的**新建数据源**，进入**项目管理 > 数据源管理**页面进行新建）<br>![](https://qcloudimg.tencent-cloud.cn/raw/f2d87334c968384e7c45bca7cfc1d830.png)| 
| 文件路径	| 文件系统的路径信息。cos 文件路径需带上桶名称，如cosn://bucket_name| 
| 文件类型	| COS 的文件类型，支持 text、ORC、Parquet、CSV| 
| <nobr>字段分隔符	| 读取的字段分隔符。支持\t、\u0001（Hive 默认）、竖线、空格、;（分号）、,（逗号）| 
| 编码	| 读取文件的编码配置，支持 utf-8、gbk| 
| 空值转换	| 选填：读取时，将指定字符串转为 null| 

####  写入

| 参数 | 说明 | 
|---------|---------|
| 数据源	| 选择已经配置好的 COS 数据源（如果未配置数据源，请单击右侧的**新建数据源**，进入**项目管理 > 数据源管理**页面进行新建）| 
| 文件路径	| 文件系统的路径信息。cos文件路径需带上桶名称，如 cosn://bucket_name| 
| 写入模式	| 数据写入方式：<li>Append 追加数据：主键/唯一性索引冲突时,会先删除原有行，再插入新行<li>Overwrite 覆盖数据：replace into 写入；主键/唯一性索引冲突时,会先删除原有行再插入新行, 相当于更新原行所有字段<li>On duplicate key：主键/唯一性索引冲突时,新行会替换已指定的字段的语句| 
| 文件类型	|COS 的文件类型，支持 text 和 CSV| 
| <nobr>字段分隔符	| 配置读取的字段分隔符，默认为,（逗号）| 
| 编码	| 读取文件的编码配置，支持 utf-8、gbk| 
| 空值转换	| 选填：读取时，将指定字符串转为 null| 

:::
::: FTP 数据源
#### 读取

| 参数 | 说明 | 
|---------|---------|
| 数据源	| 选择已经配置好的 FTP 数据源（如果未配置数据源，请单击右侧的**新建数据源**，进入**项目管理 > 数据源管理**页面进行新建）<br>![](https://qcloudimg.tencent-cloud.cn/raw/f2d87334c968384e7c45bca7cfc1d830.png)| 
| 文件路径	| 文件系统的路径信息。路径支持使用‘*’作为通配符，指定通配符后将遍历多个文件信息| 
| 文件类型	| FTP 的文件类型，当前版本仅支持 text| 
| <nobr>字段分隔符	| 配置读取的字段分隔符，默认为,（逗号）| 
| 编码	| 读取文件的编码配置，支持 utf-8、gbk| 
| 空值转换	| 选填：读取时，将指定字符串转为 null| 

####  写入

| 参数 | 说明 | 
|---------|---------|
| 数据源	| 选择已经配置好的 FTP 数据源（如果未配置数据源，请单击右侧的**新建数据源**，进入**项目管理 > 数据源管理**页面进行新建）| 
| 文件路径	| 文件系统的路径信息。路径支持使用‘*’作为通配符，指定通配符后将遍历多个文件信息| 
| 写入模式	| 数据写入方式：<li>Append 追加数据：主键/唯一性索引冲突时,会先删除原有行，再插入新行<li>Overwrite 覆盖数据：replace into 写入；主键/唯一性索引冲突时,会先删除原有行再插入新行, 相当于更新原行所有字段<li>On duplicate key：主键/唯一性索引冲突时,新行会替换已指定的字段的语句| 
| 文件类型	|FTP 的文件类型，支持 text 和 CSV| 
| <nobr>字段分隔符	| 配置读取的字段分隔符，默认为,（逗号）| 
| 编码	| 读取文件的编码配置，支持 utf-8、gbk| 
| 空值转换	| 选填：读取时，将指定字符串转为 null| 

:::
::: SFTP 数据源
#### 读取

| 参数 | 说明 | 
|---------|---------|
| 数据源	| 选择已经配置好的 SFTP 数据源（如果未配置数据源，请单击右侧的**新建数据源**，进入**项目管理 > 数据源管理**页面进行新建）<br>![](https://qcloudimg.tencent-cloud.cn/raw/f2d87334c968384e7c45bca7cfc1d830.png)| 
| 文件路径	| 文件系统的路径信息。路径支持使用‘*’作为通配符，指定通配符后将遍历多个文件信息| 
| 文件类型	| SFTP 的文件类型，当前版本仅支持 text| 
| <nobr>字段分隔符	| 配置读取的字段分隔符，默认为,（逗号）| 
| 编码	| 读取文件的编码配置，支持 utf-8、gbk| 
| 空值转换	| 选填：读取时，将指定字符串转为 null| 

####  写入

| 参数 | 说明 | 
|---------|---------|
| 数据源	| 选择已经配置好的 SFTP 数据源（如果未配置数据源，请单击右侧的**新建数据源**，进入**项目管理 > 数据源管理**页面进行新建）| 
| 文件路径	| 文件系统的路径信息。路径支持使用‘*’作为通配符，指定通配符后将遍历多个文件信息| 
| 写入模式	| 数据写入方式：<li>Append 追加数据：主键/唯一性索引冲突时,会先删除原有行，再插入新行<li>Overwrite 覆盖数据：replace into 写入；主键/唯一性索引冲突时,会先删除原有行再插入新行, 相当于更新原行所有字段<li>On duplicate key：主键/唯一性索引冲突时,新行会替换已指定的字段的语句| 
| 文件类型	|SFTP 的文件类型，支持 text 和 CSV| 
| <nobr>字段分隔符	| 配置读取的字段分隔符，默认为,（逗号）| 
| 编码	| 读取文件的编码配置，支持 utf-8、gbk| 
|  <nobr>空值转换	| 选填：读取时，将指定字符串转为 null| 

:::
::: Elasticsearch 数据源
#### 读取

| 参数 | 说明 | 
|---------|---------|
| 数据源	| 选择已经配置好的 Elasticsearch 数据源（如果未配置数据源，请单击右侧的**新建数据源**，进入**项目管理 > 数据源管理**页面进行新建）<br>![](https://qcloudimg.tencent-cloud.cn/raw/f2d87334c968384e7c45bca7cfc1d830.png)| 
| 索引	| 选择 ElasticSearch 数据源中需要进行同步任务的索引名称| 
| 切割键	| 选填：执行分库分表的切割键, 指定用于数据分片的字段，指定后将启动并发任务进行数据同步| 
|  <nobr>检索条件	| 填写 Elasticsearch 检索查询条件；默认格式：<br>```
{
"query": {
    "match": {
      "name": "xxx"
    }}}
```| 

####  写入

| 参数 | 说明 | 
|---------|---------|
| 数据源	| 选择已经配置好的 Elasticsearch 数据源（如果未配置数据源，请单击右侧的**新建数据源**，进入**项目管理 > 数据源管理**页面进行新建）| 
| 索引	| ElasticSearch 中需要同步写入的索引的名称| 
| 原索引数据	| 是否删除原索引数据，用户可选保留原索引或清理原索引数据| 
| 主键取值方式	| <li>源表主键: document的id使用源表的主键<li>联合主键: document的id使用源表的多个列共同确定| 
|  <nobr>批量提交大小	| 一次性批量提交的记录数大小，默认为1024条| 

:::
::: Kafka 数据源
#### 读取

| 参数 | 说明 | 
|---------|---------|
| 数据源	| 选择已经配置好的 Kafka 数据源（如果未配置数据源，请单击右侧的**新建数据源**，进入**项目管理 > 数据源管理**页面进行新建）<br>![](https://qcloudimg.tencent-cloud.cn/raw/f2d87334c968384e7c45bca7cfc1d830.png)| 
| topic	| 选择 Kafka 数据源中的需要进行同步任务的 Topic| 
|  <nobr>序列化格式	| Kafka 消息序列化格式类型，支持 AVRO、JSON| 
|  <nobr>读取位置	| 启动同步任务时开始同步数据的起始位点<li>Latest: 从最新位置开始读取，忽略历史消息(startingOffset=latest)<li>Earliest: 从最开始位置开始读取 (startingOffset=earliest)| 

:::
::: Redis 数据源
#### 写入

| 参数 | 说明 | 
|---------|---------|
| 数据源	| 选择已经配置好的 Redis 数据源（如果未配置数据源，请单击右侧的**新建数据源**，进入**项目管理 > 数据源管理**页面进行新建）<br>![](https://qcloudimg.tencent-cloud.cn/raw/f2d87334c968384e7c45bca7cfc1d830.png)| 
| 库	| 选取需要同步的目标数据库
| 键分隔符	| 支持\u0001（Hive 默认）、冒号、逗号、竖线| 
| 值类型	| 选择value类型，支持string、list、set、zset、hash| 
| 值分隔符	| 支持\u0001（Hive 默认）、冒号、逗号、竖线| 

:::
</dx-accordion>

