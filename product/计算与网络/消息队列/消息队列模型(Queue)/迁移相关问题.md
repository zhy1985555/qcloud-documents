以下 TDMQ CMQ 版简称为新版 CMQ，当前的消息队列 CMQ 简称为原 CMQ。

### 基础数据迁移相关

#### 问题场景：
单击 CMQ 界面上的基础数据后发现新 CMQ 订阅的数量和原 CMQ 的订阅数量不符。

#### 问题说明：
其实相差的这部分是曾经在原 CMQ 已经被人为删除了的队列，而原 CMQ 在查看订阅时不会严格过滤已经删除的队列，实际上这些“多余”的订阅者早已失去了相关的消费逻辑。但是在新 CMQ 的订阅逻辑里，只要队列删除了，相关的主题订阅关系也一并会被删除，因此不会展示这部分订阅者。

#### 解决方案：
如果您希望验证这个问题，可以将原 CMQ 里“没有迁移过来”的订阅队列名输入到原 CMQ 队列列表的搜索框中搜索一下，没有结果则证明该订阅关系在新 CMQ 的缺失属于上述情况。


### 消费时延明显增长

#### 问题场景：
切换到新版 CMQ 后发现消息拉取的时间明显变长（可能达到10s - 20s）。此类问题属于已知问题，常出现在消息量不大，且消费者不多的场景。原因在于新版的分布式架构中，拉取消息轮询各节点的随机性在消息数量少的时候体现得更为明显，使得底层会有一些反复轮询的现象，导致时延增加。

#### 解决方案：

1. 队列属性中的“消息接收长轮询等待时间”改小，推荐调整为3秒。
2. 我们准备了专门针对这种场景特化的集群，降低轮询耗时，**用户将客户端的接入点切换为这些集群即可明显降低消息拉取的时耗**（无需重建队列）。

目前已支持特化集群的有以下地域：

|地域|接入点|
|-|-|
|广州| <li>http://gz.mqadapter.cmq.tencentyun.com:8080(内网)</li><li>https://cmq-gz.public.tencenttdmq.com:8443(公网)</li><li>https://cmq-gz.public.tencenttdmq.com:9443</li>(公网，HTTPS专用)|
|上海| <li>http://sh.mqadapter.cmq.tencentyun.com:8080(内网)</li><li>https://cmq-sh.public.tencenttdmq.com:8443(公网)</li>|
|北京| <li>http://bj.mqadapter.cmq.tencentyun.com:8080(内网)</li><li>https://cmq-bj.public.tencenttdmq.com:8443(公网)</li>|
|中国香港| <li>http://hk.mqadapter.cmq.tencentyun.com:8080(内网)</li><li>https://cmq-hk.public.tencenttdmq.com:8443(公网)</li>|


### 管控类 API(新增/查看队列等)不兼容
#### 问题场景：
切换过来后 SDK 中 createQueue 等管控类的方法报错。

#### 问题说明：
新版 CMQ 对于原来 SDK 中管控流的操作是不兼容的（创建队列、查看队列列表等接口）。 腾讯云对控制台整体的云 API 做了一次升级（V2 到 V3），不再提供原来 V2 协议的接口，因此新版 CMQ 管控流相关的接口需要遵照最新云 API 的协议进行改造，改造后性能和开发友好性更高。具体可以参见 [API 文档](https://cloud.tencent.com/document/product/1496/62819)。

新版 CMQ 管控流和数据流做了分离，原则上是这两类的调用场景和性质不同，区分可以更好提供服务，因此调用的域名也不相同：
数据流的调用地址请在控制台获取；管控流的调用地址请参考具体的云 API 的文档，例如 [创建 CMQ 队列](https://cloud.tencent.com/document/api/1179/55917) API 文档。

### 原先使用的 TCP 协议的 SDK，新版 CMQ 出错

新版 CMQ 默认仅支持 HTTP 协议，如果原先有使用 TCP 协议的 SDK 且希望无缝迁移，我们提供了 TCP  SDK 的专属接入点，如下所示：

|地域|接入点|
|-|-|
|广州| <li>http://gz.mqadapter.cmq.tencentyun.com:12000 (内网)</li><li>https://cmq-gz.public.tencenttdmq.com:12000 (公网)</li>|
|上海| <li>http://sh.mqadapter.cmq.tencentyun.com:12000 (内网)</li><li>https://cmq-sh.public.tencenttdmq.com:12000 (公网)</li>|
|北京| <li>http://bj.mqadapter.cmq.tencentyun.com:12000 (内网)</li><li>https://cmq.bj.public.tencenttdmq.com:12000 (公网)</li>|

TCP 的客户端配置成 TCP 专用的接入点即可解决。


### 存量系统复杂迁移困难

如果您发现您正在使用 CMQ 的系统较为复杂或由于各类原因难以维护，导致迁移困难，请及时通过 [工单](https://console.cloud.tencent.com/workorder/category?level1_id=876&level2_id=947&source=14&data_title=%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%20CMQ&step=1) 与我们联系，我们将提供后台切换的方案协助您进行迁移。


<dx-alert infotype="notice" title="">
<li>后台切换会有一定的延时增长</li>
<li>后台切换完成后，新增或者编辑的操作请勿在原版 CMQ 进行</li>
</dx-alert>



