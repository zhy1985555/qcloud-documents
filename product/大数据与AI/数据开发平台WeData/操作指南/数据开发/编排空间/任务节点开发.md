任务节点开发支持多种任务类型，以满足不同的开发应用场景。
## 数据同步任务
离线同步任务分别支持23种写入数据源和23种读取数据源，详见数据集成中 [离线同步任务](https://cloud.tencent.com/document/product/1267/72364)。

## 计算任务
### Hive SQL 任务
您需要在项目下绑定包含 Hive 组件的 EMR 引擎即可使用 Hive SQL 任务节点。您可以创建 Hive SQL 任务节点，通过类 SQL 语句进行大数据集的分析、开发处理工作。
操作步骤如下：
1. 进入数据开发页面
	1. 登录腾讯云 WeData 数据开发平台。
	2. 单击左侧目录树项目列表。
	3. 选择对应的项目，并单击进入数据开发模块。
	4. 单击左侧目录树中编排空间。

2. 创建 Hive SQL 任务节点。
	1. 选择对应工作流，单击右键选择**新建任务 > Hive SQL 任务**。
![](https://qcloudimg.tencent-cloud.cn/raw/0c9d368c987734dc6f353bb75e669070.png)
	2. 在弹窗中填写任务名称，单击**确认**。选择**自动创建**会在对应工作流下创建任务，选择**手动选择**则可以自定义任务路径。
![](https://qcloudimg.tencent-cloud.cn/raw/ddd8d5d3e6eae020b6fbfe6c756065d1.png)

3. 创建成功后，在编辑页面中输入代码。
4. 编辑任务配置。
单击任务右侧**任务属性**、**调度设置**，在面板中配置任务的基本信息、调度信息
![](https://qcloudimg.tencent-cloud.cn/raw/968db14a89a7424bcb72b29c2ef3a214.png)
>? 
>- 运行参数：如 SQL 中有变量参数，请进行赋值，如 key0=123，多个参数用英文分号“;”分隔。
>- 特别参数：如 SQL 中有特别指定的参数，如 set hive.exec.parallel = true，多个参数用英文分号。

5. 选择数据源、资源队列、执行资源组。
6. 保存并提交。

### Spark SQL 任务
您需要在项目下绑定包含 Spark 组件的 EMR 引擎即可使用 EMR Spark SQL 任务节点。您可以创建 EMR Spark SQL 任务节点，通过类 SQL 语句进行大数据集的分析、开发处理工作。
操作步骤同 Hive SQL 任务。

### PostgreSQL 任务
您需要在项目下绑定 CDW PostgreSQL 引擎即可使用 PostgreSQL 任务节点。
操作步骤同 Hive SQL 任务。

### Shell 任务
开发空间和编排空间支持Shell脚本任务。
操作步骤如下：
1. 进入数据开发页面。
	1. 登录腾讯云 WeData 数据开发平台。
	2. 单击左侧目录树项目列表。
	3. 选择对应的项目，并单击进入数据开发模块。
	4. 单击左侧目录树中编排空间。
2. 创建 shell 任务节点。
	1. 选择对应工作流，单击右键选择**新建任务 > Shell 任务**。
	2. 在弹窗中填写任务名称，单击**确认**。

3. 在编辑页面中输入代码。
Shell 任务支持代码和脚本两种编辑模式，通过点击 在两种模式之间进行切换。
	- 代码模式
![](https://qcloudimg.tencent-cloud.cn/raw/01604f2ec39885cfd28d133131cfc906.png)
	- 脚本模式
![](https://qcloudimg.tencent-cloud.cn/raw/319d8d0846140c636628e0b512bb48b7.png)


| 信息 | 详情 | 
|---------|---------|
| Shell 任务执行资源（zip）	| 任务执行资源（zip），不要将资源放到子目录| 
| Shell 命令	| 要执行的shell脚本的名字(如：script.sh)，若在脚本中执行 beeline 命令,需要在 runner 节点安装 tez client| 
| Shell 参数	| 要执行的脚本的参数，用空格区分多个参数| 

4. 编辑任务配置
单击任务右侧**任务配置**，在面板中设置任务配置的基本信息、调度信息及告警方式。
>? 
>- 运行参数：如 SQL 中有变量参数，请进行赋值，如 key0=123，多个参数用英文分号“;”分隔。
>- 特别参数：如 SQL 中有特别指定的参数，如 set hive.exec.parallel = true，多个参数用英文分号。

5. 选择并指定执行资源主机。
6. 保存并提交当前shell任务。

### JDBC SQL 任务
功能说明：向 WeData 的工作流调度系统提交 SQL 命令进行调度执行，仅支持 oracle 类型。
![](https://qcloudimg.tencent-cloud.cn/raw/d7615d1dfc697fc6b2761e2f609a8522.png)
参数说明：

| 参数 | 说明 | 
|---------|---------|
| 数据源类型	| 仅支持 oracle 数据源类型| 
| 数据源	| 执行 SQL 任务的数据源信息，需在数据管理>数据源管理中事先创建完成| 
| Sql 编辑	| 编辑 sql 代码，支持保存、提交、测试运行、版本管理等| 

其余参数设定参见 Hive SQL 任务说明。

### MapReduce 任务
功能说明：向 WeData 的工作流调度平台提交一个 MapReduce 任务执行。
![](https://qcloudimg.tencent-cloud.cn/raw/2d0e9be61a1bec1796179c4997107aae.png)
参数说明：

| 参数 | 说明 | 
|---------|---------|
| 输出目录	| MR 任务的文件或者结果输出目录（如有），这里写入的目录工作流任务在调度时会主动清空该目录|
| 前置 check 命令	| 执行 MR 任务之前，用户可以执行前置命令，例如判断 HDFS 数据目录是否存在，或者执行一些前置 shell 命令（执行 shell 脚本），可以输入多个前置命令| 
| 命令脚本	| MR 任务的具体代码实现，所有用到依赖全部打包为 zip 文件，不要打包目录，直接打包文件。
| 命令行参数		| 执行 MR 任务用到的相关参数| 
| 后置 done 命令	| 任务执行后的 done 命令，在 HDFS 中 touchz 一个文件或执行清理脚本。对在 HDFS 中 touchz 一个文件，采用“done hdfsdir”； 对于清理脚本，采用“*.sh argv”| 
| Check 命令超时	| 前置 check 命令如果存在多个，每个前置命令执行时间不超过5min，总执行时间不超过该设置值（min）。可选值有：5、10分钟| 
| <nobr>任务超时（分钟）	| 提交 MR 任务的超时时间，超过设置的时间，任务会被终止。默认为300分钟，可选值有：180、200、270、300、360、600| 

其余参数设定参考 Hive SQL 任务说明。

### PySpark 任务
功能说明：使用 Python 在线编写 Spark 程序，并提交到 WeData 的工作流调度系统进行调度执行。
![](https://qcloudimg.tencent-cloud.cn/raw/be19fd57d9260df7abf72160b33e8099.png)
参数说明：

| 参数 | 说明 | 
|---------|---------|
| Python 版本 | 支持 Python2、Python3 | 

其余参数设定参考 Hive SQL 任务说明。

### Python 任务
功能说明：在节点上编辑 Python 代码，并支持向 WeData 的工作流调度系统提交命令进行调度执行。
![](https://qcloudimg.tencent-cloud.cn/raw/be19fd57d9260df7abf72160b33e8099.png)
参数说明：

| 参数 | 说明 | 
|---------|---------|
| Python 版本 | 支持 Python2和 Python3 | 

其余参数设定参考 Hive SQL 任务说明。

### Spark 任务
功能说明：向 WeData 的工作流调度平台提交一个 Spark 任务执行。

参数说明：

| 参数 | 说明 | 
|---------|---------|
| spark 程序 zip 包	| 用户直接上传编写的 spark 程序代码文件，需要打包为 jar 后，将所有自定义的依赖打包为一个 zip 文件，不要打包目录，直接打包文件本身| 
| 执行参数	| spark 程序的执行参数，无需用户写 spark-submit，无需指定提交用户，无需指定提交队列，无需指定提交模式（默认为 yarn）。参数格式如：--class mainClass run.jar args 或 wordcount.py input output| 
| 应用参数	| spark 的应用参数| 

其余参数设定参考 Hive SQL 任务说明。

### DLC 任务
功能说明：向 WeData 的工作流调度平台提交一个 DLC 任务执行。
![](https://qcloudimg.tencent-cloud.cn/raw/d39592d36a6c64092940d8b14275bd4c.png)
其余参数设定参考 Hive SQL 任务说明。

### Impala 任务
功能说明：向 WeData 的工作流调度平台提交一个 Impala 任务执行。
![](https://qcloudimg.tencent-cloud.cn/raw/dced02274be70b7a3471b89688dcf1a6.png)
其余参数设定参考 Hive SQL 任务说明。

### CDW PG 任务
功能说明：向 WeData 的工作流调度平台提交一个 CDW PG 任务执行。
![](https://qcloudimg.tencent-cloud.cn/raw/9b88e9037690621337caf32ec2f9a591.png)
其余参数设定参考 Hive SQL 任务说明。


## 跨工作流任务
跨工作流任务主要用于不同工作流之间的联动，允许在不同工作流之间建立以来关系。跨工作流任务只能作为父任务，仅可对其创建下游依赖任务。跨工作流任务的操作步骤如下：
1. 进入数据开发页面。
	1. 登录腾讯云 WeData 数据开发平台。
	2. 单击左侧目录树项目列表。
	3. 选择对应的项目，并单击进入数据开发模块。
	4. 单击左侧目录树中编排空间。
2. 创建跨工作流任务节点。
	1. 选择对应工作流，单击右键选择**新建任务 >通用**（跨工作流节点）。
![](https://qcloudimg.tencent-cloud.cn/raw/3ed0576b196dc2642674316d7b9081d8.png)
	2. 在弹窗中选择要依赖的工作流及任务，单击**完成**。
![](https://qcloudimg.tencent-cloud.cn/raw/4799a43a0c72cf4ffbfb4db9414b1d96.png)
![](https://qcloudimg.tencent-cloud.cn/raw/553f7fe4a47989e71027e23878eed048.png)

3. 配置任务间依赖关系。
>? 跨工作流任务仅可作为父节点。
